{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Alireza Zare Z. 9931022 NN-CI-Spring2023**"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "tLdLlSB7aOh3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from abc import ABC, abstractmethod\n",
        "import pickle\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from random import shuffle\n",
        "import csv\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "mnist_path = {2: '/content/drive/MyDrive/datasets/MNIST/2/', 5: '/content/drive/MyDrive/datasets/MNIST/5/'}\n",
        "cali_path = {'train': '/content/drive/MyDrive/datasets/california_houses_price/california_housing_train.csv','test': '/content/drive/MyDrive/datasets/california_houses_price/california_housing_test.csv'}\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGVgKgPHaOh9",
        "outputId": "09ef0a12-7113-4cee-b502-97bbd148367a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this code, a class named `FC` is defined, representing a fully connected layer in a neural network.\n",
        "\n",
        "2. The `initialize_weights` method initializes the weights. If the initialization method is set to \"random\", the weights are initialized using a normal distribution with `np.random.randn` function.\n",
        "\n",
        "3. The `initialize_bias` method initializes the bias with zero values. The bias is a numpy array of shape `(output_size, 1)`.\n",
        "\n",
        "4. The `forward` method is used for the forward pass in the fully connected layer. It takes the input `A_prev` and computes the output of the fully connected layer, returning the result.\n",
        "\n",
        "5. The `backward` method is used for the backward pass in the fully connected layer. It takes the derivatives of the cost with respect to the current layer's output (`dZ`) and the activations from the previous layer (`A_prev`), and returns the derivative of the cost with respect to the activation of the previous layer (`dA_prev`) and a list of gradients for the weights and bias.\n",
        "\n",
        "6. The `update_parameters` method updates the layer's parameters using an optimizer. It takes an optimizer object and a list of gradients for the weights and bias, and updates the parameters using the optimizer."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "0Lbb8NnMaOh-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class FC:\n",
        "    def __init__(self, input_size : int, output_size : int, name : str, initialize_method : str=\"random\"):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.name = name\n",
        "        self.initialize_method = initialize_method\n",
        "        self.parameters = [self.initialize_weights(), self.initialize_bias()]\n",
        "        self.input_shape = None\n",
        "        self.reshaped_shape = None\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        if self.initialize_method == \"random\":\n",
        "            # TODO: Initialize weights with random values using np.random.randn\n",
        "            return np.random.randn(self.output_size, self.input_size) * 0.01\n",
        "\n",
        "        elif self.initialize_method == \"xavier\":\n",
        "            return None\n",
        "\n",
        "        elif self.initialize_method == \"he\":\n",
        "            return None\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid initialization method\")\n",
        "\n",
        "    def initialize_bias(self):\n",
        "        # TODO: Initialize bias with zeros\n",
        "        return np.zeros((self.output_size, 1))\n",
        "\n",
        "    def forward(self, A_prev):\n",
        "        \"\"\"\n",
        "        Forward pass for fully connected layer.\n",
        "            args:\n",
        "                A_prev: activations from previous layer (or input data)\n",
        "                A_prev.shape = (batch_size, input_size)\n",
        "            returns:\n",
        "                Z: output of the fully connected layer\n",
        "        \"\"\"\n",
        "        # NOTICE: BATCH_SIZE is the first dimension of A_prev\n",
        "        self.input_shape = A_prev.shape\n",
        "        A_prev_tmp = np.copy(A_prev)\n",
        "\n",
        "        # TODO: Implement forward pass for fully connected layer\n",
        "        if len(A_prev.shape) > 2: # check if A_prev is output of convolutional layer\n",
        "            batch_size = A_prev.shape[0]\n",
        "            A_prev_tmp = A_prev_tmp.reshape(batch_size, -1).T\n",
        "        self.reshaped_shape = A_prev_tmp.shape\n",
        "\n",
        "        # TODO: Forward part\n",
        "        W, b = self.parameters\n",
        "        Z = W @ A_prev_tmp + b\n",
        "        return Z\n",
        "\n",
        "    def backward(self, dZ, A_prev):\n",
        "        \"\"\"\n",
        "        Backward pass for fully connected layer.\n",
        "            args:\n",
        "                dZ: derivative of the cost with respect to the output of the current layer\n",
        "                A_prev: activations from previous layer (or input data)\n",
        "            returns:\n",
        "                dA_prev: derivative of the cost with respect to the activation of the previous layer\n",
        "                grads: list of gradients for the weights and bias\n",
        "        \"\"\"\n",
        "        A_prev_tmp = np.copy(A_prev)\n",
        "        if len(A_prev.shape) > 2: # check if A_prev is output of convolutional layer\n",
        "            batch_size = A_prev.shape[0]\n",
        "            A_prev_tmp = A_prev_tmp.reshape(batch_size, -1).T\n",
        "\n",
        "        # TODO: backward part\n",
        "        W, b = self.parameters\n",
        "        dW = dZ @ A_prev_tmp.T / A_prev_tmp.shape[1]\n",
        "        db = np.sum(dZ, axis=1, keepdims=True) / A_prev_tmp.shape[1]\n",
        "        dA_prev = W.T @ dZ\n",
        "        grads = [dW, db]\n",
        "        # reshape dA_prev to the shape of A_prev\n",
        "        if len(A_prev.shape) > 2:    # check if A_prev is output of convolutional layer\n",
        "            dA_prev = dA_prev.T.reshape(self.input_shape)\n",
        "        return dA_prev, grads\n",
        "\n",
        "    def update_parameters(self, optimizer, grads):\n",
        "        \"\"\"\n",
        "        Update the parameters of the layer.\n",
        "            args:\n",
        "                optimizer: optimizer object\n",
        "                grads: list of gradients for the weights and bias\n",
        "        \"\"\"\n",
        "        self.parameters = optimizer.update(grads, self.name)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "FRlvLV_gaOh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code defines a class `Conv2D` representing a 2D convolutional layer in a neural network.\n",
        "\n",
        "2. The `initialize_weights` method initializes the kernel weights. If the initialization method is set to \"random\", the weights are initialized using a normal distribution with `np.random.randn` function.\n",
        "\n",
        "3. The `initialize_bias` method initializes the bias with zero values. The bias is a numpy array of shape `(1, 1, 1, out_channels)`.\n",
        "\n",
        "4. The `target_shape` method calculates the shape of the output of the convolutional layer based on the input shape.\n",
        "\n",
        "5. The `pad` method pads the input with zeros using the specified padding value and padding dimensions.\n",
        "\n",
        "6. The `single_step_convolve` method performs a single step of convolution by taking a slice of the input, the kernel weights, and bias, and returns the convolved value.\n",
        "\n",
        "7. The `forward` method implements the forward pass of the convolutional layer. It takes the input `A_prev` and computes the output of the convolutional layer using the kernel weights and bias.\n",
        "\n",
        "8. The `backward` method implements the backward pass of the convolutional layer. It takes the gradient of the cost with respect to the output of the convolutional layer (`dZ`) and the activations from the previous layer (`A_prev`), and returns the gradient of the cost with respect to the input of the convolutional layer (`dA_prev`) and a list of gradients for the weights and bias.\n",
        "\n",
        "9. The `update_parameters` method updates the layer's parameters using an optimizer. It takes an optimizer object and a list of gradients for the weights and bias, and updates the parameters using the optimizer."
      ],
      "metadata": {
        "id": "B-qyWk96-o5e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class Conv2D:\n",
        "    def __init__(self, in_channels, out_channels, name, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), initialize_method=\"random\"):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.name = name\n",
        "        self.initialize_method = initialize_method\n",
        "\n",
        "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
        "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
        "        self.padding = (padding, padding) if isinstance(padding, int) else padding\n",
        "        self.parameters = [self.initialize_weights(), self.initialize_bias()]\n",
        "\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize weights.\n",
        "        returns:\n",
        "            weights: initialized kernel with shape: (kernel_size[0], kernel_size[1], in_channels, out_channels)\n",
        "        \"\"\"\n",
        "        # TODO: Implement initialization of weights\n",
        "\n",
        "        if self.initialize_method == \"random\":\n",
        "            return np.random.randn(self.kernel_size[0], self.kernel_size[1], self.in_channels, self.out_channels) * 0.01\n",
        "        if self.initialize_method == \"xavier\":\n",
        "            return None\n",
        "        if self.initialize_method == \"he\":\n",
        "            return None\n",
        "        else:\n",
        "            raise ValueError(\"Invalid initialization method\")\n",
        "\n",
        "    def initialize_bias(self):\n",
        "        \"\"\"\n",
        "        Initialize bias.\n",
        "        returns:\n",
        "            bias: initialized bias with shape: (1, 1, 1, out_channels)\n",
        "\n",
        "        \"\"\"\n",
        "        # TODO: Implement initialization of bias\n",
        "        return np.zeros((1, 1, 1, self.out_channels))\n",
        "\n",
        "\n",
        "    def target_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "        Calculate the shape of the output of the convolutional layer.\n",
        "        args:\n",
        "            input_shape: shape of the input to the convolutional layer\n",
        "        returns:\n",
        "            target_shape: shape of the output of the convolutional layer\n",
        "        \"\"\"\n",
        "        # TODO: Implement calculation of target shape\n",
        "        h = input.shape[1]\n",
        "        w = input.shape[2]\n",
        "        H = int((h + 2*self.padding[0] - self.kernel_size[0]) / self.stride[0]) + 1\n",
        "        W = int((w + 2*self.padding[1] - self.kernel_size[1]) / self.stride[1]) + 1\n",
        "        return (H, W)\n",
        "\n",
        "    def pad(self, A, padding, pad_value=0):\n",
        "        \"\"\"\n",
        "        Pad the input with zeros.\n",
        "        args:\n",
        "            A: input to be padded\n",
        "            padding: tuple of padding for height and width\n",
        "            pad_value: value to pad with\n",
        "        returns:\n",
        "            A_padded: padded input\n",
        "        \"\"\"\n",
        "        A_padded = np.pad(A, ((0, 0), (padding[0], padding[0]), (padding[1], padding[1]), (0, 0)), mode=\"constant\", constant_values=(pad_value, pad_value))\n",
        "        return A_padded\n",
        "\n",
        "    def single_step_convolve(self, a_slic_prev, W, b):\n",
        "        \"\"\"\n",
        "        Convolve a slice of the input with the kernel.\n",
        "        args:\n",
        "            a_slic_prev: slice of the input data\n",
        "            W: kernel\n",
        "            b: bias\n",
        "        returns:\n",
        "            Z: convolved value\n",
        "        \"\"\"\n",
        "        # TODO: Implement single step convolution\n",
        "        Z = np.multiply(a_slic_prev, W)   # hint: element-wise multiplication\n",
        "        Z = np.sum(Z)   # hint: sum over all elements\n",
        "        Z = np.float(Z + b)    # hint: add bias as type float using np.float(None)\n",
        "        return Z\n",
        "\n",
        "    def forward(self, A_prev):\n",
        "        \"\"\"\n",
        "        Forward pass for convolutional layer.\n",
        "            args:\n",
        "                A_prev: activations from previous layer (or input data)\n",
        "                A_prev.shape = (batch_size, H_prev, W_prev, C_prev)\n",
        "            returns:\n",
        "                A: output of the convolutional layer\n",
        "        \"\"\"\n",
        "        # TODO: Implement forward pass\n",
        "        W, b = self.parameters\n",
        "        (batch_size, H_prev, W_prev, C_prev) = A_prev.shape\n",
        "        (kernel_size_h, kernel_size_w, C_prev, C) = W.shape\n",
        "        stride_h, stride_w = self.stride\n",
        "        padding_h, padding_w = self.padding\n",
        "        H, W = self.target_shape(A_prev.shape)\n",
        "        Z = np.zeros((batch_size, H, W, C))\n",
        "        A_prev_pad = self.pad(A_prev, (padding_h, padding_w)) # hint: use self.pad()\n",
        "        for i in range(batch_size):\n",
        "            for h in range(H):\n",
        "                h_start = h * stride_h\n",
        "                h_end = h_start + kernel_size_h\n",
        "                for w in range(W):\n",
        "                    w_start = w * stride_w\n",
        "                    w_end = w_start + kernel_size_w\n",
        "                    for c in range(C):\n",
        "                        a_slic_prev = A_prev_pad[i, h_start:h_end, w_start:w_end, :]\n",
        "                        Z[i, h, w, c] = self.single_step_convolve(a_slic_prev, W[:, :, :, c], b[:, :, : c]) # hint: use self.single_step_convolve()\n",
        "        return Z\n",
        "\n",
        "    def backward(self, dZ, A_prev):\n",
        "        \"\"\"\n",
        "        Backward pass for convolutional layer.\n",
        "        args:\n",
        "            dZ: gradient of the cost with respect to the output of the convolutional layer\n",
        "            A_prev: activations from previous layer (or input data)\n",
        "            A_prev.shape = (batch_size, H_prev, W_prev, C_prev)\n",
        "        returns:\n",
        "            dA_prev: gradient of the cost with respect to the input of the convolutional layer\n",
        "            gradients: list of gradients with respect to the weights and bias\n",
        "        \"\"\"\n",
        "        # TODO: Implement backward pass\n",
        "        W, b = self.parameters\n",
        "        (batch_size, H_prev, W_prev, C_prev) = A_prev.shape\n",
        "        (kernel_size_h, kernel_size_w, C_prev, C) = W.shape\n",
        "        stride_h, stride_w = self.stride\n",
        "        padding_h, padding_w = self.padding\n",
        "        H, W = dZ.shape[1:3]\n",
        "        dA_prev = np.zeros((batch_size, H_prev, W_prev, C_prev))  # hint: same shape as A_prev\n",
        "        dW = np.zeros_like(W)    # hint: same shape as W\n",
        "        db = np.zeros_like(b)    # hint: same shape as b\n",
        "        A_prev_pad = self.pad(A_prev, (padding_h, padding_w)) # hint: use self.pad()\n",
        "        dA_prev_pad = self.pad(dA_prev, (padding_h, padding_w)) # hint: use self.pad()\n",
        "        for i in range(batch_size):\n",
        "            a_prev_pad = A_prev_pad[i]\n",
        "            da_prev_pad = dA_prev_pad[i]\n",
        "            for h in range(H):\n",
        "                for w in range(W):\n",
        "                    for c in range(C):\n",
        "                        h_start = h * stride_h\n",
        "                        h_end = h_start + kernel_size_h\n",
        "                        w_start = w * stride_w\n",
        "                        w_end = w_start + kernel_size_w\n",
        "                        a_slice = a_prev_pad[h_start:h_end, w_start:w_end, :]\n",
        "                        da_prev_pad[h_start:h_end, w_start:w_end, :] += W[..., c] / dZ[i, h, w, c] # hint: use element-wise multiplication of dZ and W\n",
        "                        dW[..., c] += a_slice * dZ[i, h, w, c]# hint: use element-wise multiplication of dZ and a_slice\n",
        "                        db[..., c] += dZ[i, h, w, c] # hint: use dZ\n",
        "            dA_prev[i, :, :, :] = da_prev_pad[padding_h:-padding_h, padding_w:-padding_w, :] # hint: remove padding (trick: pad:-pad)\n",
        "        grads = [dW, db]\n",
        "        return dA_prev, grads\n",
        "\n",
        "    def update_parameters(self, optimizer, grads):\n",
        "        \"\"\"\n",
        "        Update parameters of the convolutional layer.\n",
        "        args:\n",
        "            optimizer: optimizer to use for updating parameters\n",
        "            grads: list of gradients with respect to the weights and bias\n",
        "        \"\"\"\n",
        "        self.parameters = optimizer.update(grads, self.name)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "vs5iw8EgaOiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The `MaxPool2D` class represents a max pooling layer. Here's a breakdown of the different methods:\n",
        "\n",
        "1. `__init__(self, kernel_size=(3, 3), stride=(1, 1), mode=\"max\")`: This method is the constructor of the `MaxPool2D` class. It initializes the layer with the provided `kernel_size`, `stride`, and `mode` parameters. The `stride` and `kernel_size` are converted to tuples if they are provided as single integers.\n",
        "\n",
        "2. `target_shape(self, input_shape)`: This method calculates the shape of the output of the layer based on the input shape. It uses the formula `(input_shape - kernel_size) / stride + 1` to determine the height and width of the output.\n",
        "\n",
        "3. `forward(self, A_prev)`: This method performs the forward pass for the max pooling layer. It takes the activations from the previous layer (or input data) as input (`A_prev`) and computes the output of the max pooling layer. It iterates over the input data and applies the max pooling operation based on the `kernel_size`, `stride`, and `mode` parameters.\n",
        "\n",
        "4. `create_mask_from_window(self, x)`: This method creates a mask from an input matrix `x` to identify the maximum entry. It compares each element of `x` with the maximum value and sets the corresponding element in the mask to `True`.\n",
        "\n",
        "5. `distribute_value(self, dz, shape)`: This method distributes the input value `dz` in a matrix of dimension `shape`. It calculates the average value based on the dimensions of the matrix and distributes it to each element.\n",
        "\n",
        "6. `backward(self, dZ, A_prev)`: This method performs the backward pass for the max pooling layer. It takes the gradient of the cost with respect to the output of the max pooling layer (`dZ`) and the activations from the previous layer (`A_prev`) as inputs. It computes the gradient of the cost with respect to the input of the max pooling layer (`dA_prev`) using the chain rule and the mask created during the forward pass."
      ],
      "metadata": {
        "id": "3RN1fHMs-lQu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class MaxPool2D:\n",
        "    def __init__(self, kernel_size=(3, 3), stride=(1, 1), mode=\"max\"):\n",
        "        \"\"\"\n",
        "        Max pooling layer.\n",
        "            args:\n",
        "                kernel_size: size of the kernel\n",
        "                stride: stride of the kernel\n",
        "                mode: max or average\n",
        "            Question:Why we don't need to set name for the layer?\n",
        "        \"\"\"\n",
        "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
        "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
        "        self.mode = mode\n",
        "\n",
        "    def target_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "        Calculate the shape of the output of the layer.\n",
        "            args:\n",
        "                input_shape: shape of the input\n",
        "            returns:\n",
        "                output_shape: shape of the output\n",
        "        \"\"\"\n",
        "        # TODO: Implement target_shape\n",
        "        H = int((input_shape[1] - self.kernel_size[0]) / self.stride[0] + 1)\n",
        "        W = int((input_shape[2] - self.kernel_size[1]) / self.stride[1] + 1)\n",
        "        return H, W\n",
        "\n",
        "    def forward(self, A_prev):\n",
        "        \"\"\"\n",
        "        Forward pass for max pooling layer.\n",
        "            args:\n",
        "                A_prev: activations from previous layer (or input data)\n",
        "            returns:\n",
        "                A: output of the max pooling layer\n",
        "        \"\"\"\n",
        "        # TODO: Implement forward pass for max pooling layer\n",
        "        (batch_size, H_prev, W_prev, C_prev) = A_prev.shape\n",
        "        (f_h, f_w) = self.kernel_size\n",
        "        strideh, stridew = self.stride\n",
        "        H, W = self.target_shape(A_prev.shape)\n",
        "        A = np.zeros((batch_size, H, W, C_prev))\n",
        "        for i in range(batch_size):\n",
        "            for h in range(H):\n",
        "                h_start = h * strideh\n",
        "                h_end = h_start + f_h\n",
        "                for w in range(W):\n",
        "                    w_start = w * stridew\n",
        "                    w_end = w_start + f_w\n",
        "                    for c in range(C_prev):\n",
        "                        a_prev_slice = A_prev[i, h_start:h_end, w_start:w_end, c]\n",
        "                        if self.mode == \"max\":\n",
        "                            A[i, h, w, c] = np.max(a_prev_slice * self.create_mask_from_window(a_prev_slice))\n",
        "                        elif self.mode == \"average\":\n",
        "                            A[i, h, w, c] = np.mean(a_prev_slice)\n",
        "                        else:\n",
        "                            raise ValueError(\"Invalid mode\")\n",
        "\n",
        "        return A\n",
        "\n",
        "    def create_mask_from_window(self, x):\n",
        "        \"\"\"\n",
        "        Create a mask from an input matrix x, to identify the max entry of x.\n",
        "            args:\n",
        "                x: numpy array\n",
        "            returns:\n",
        "                mask: numpy array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
        "        \"\"\"\n",
        "        # TODO: Implement create_mask_from_window\n",
        "        mask = x == np.max(x)\n",
        "        return mask\n",
        "\n",
        "    def distribute_value(self, dz, shape):\n",
        "        \"\"\"\n",
        "        Distribute the input value in the matrix of dimension shape.\n",
        "            args:\n",
        "                dz: input scalar\n",
        "                shape: the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
        "            returns:\n",
        "                a: distributed value\n",
        "        \"\"\"\n",
        "        # TODO: Implement distribute_value\n",
        "        (n_H, n_W) = shape\n",
        "        average = dz/ (n_H * n_W)\n",
        "        a = np.ones(shape) * average\n",
        "        return a\n",
        "\n",
        "    def backward(self, dZ, A_prev):\n",
        "        \"\"\"\n",
        "        Backward pass for max pooling layer.\n",
        "            args:\n",
        "                dA: gradient of cost with respect to the output of the max pooling layer\n",
        "                A_prev: activations from previous layer (or input data)\n",
        "            returns:\n",
        "                dA_prev: gradient of cost with respect to the input of the max pooling layer\n",
        "        \"\"\"\n",
        "        # TODO: Implement backward pass for max pooling layer\n",
        "        (f_h, f_w) = self.kernel_size\n",
        "        strideh, stridew = self.stride\n",
        "        batch_size, H_prev, W_prev, C_prev = A_prev.shape\n",
        "        batch_size, H, W, C = dZ.shape\n",
        "        dA_prev = np.zeros((batch_size, H_prev, W_prev, C_prev))\n",
        "        for i in range(batch_size):\n",
        "            for h in range(H):\n",
        "                for w in range(W):\n",
        "                    for c in range(C):\n",
        "                        h_start = h * strideh\n",
        "                        h_end = h_start + f_h\n",
        "                        w_start = w * strideh\n",
        "                        w_end = w_start + f_w\n",
        "                        if self.mode == \"max\":\n",
        "                            a_prev_slice = A_prev[i, h_start:h_end, w_start:w_end, c]\n",
        "                            mask = self.create_mask_from_window(a_prev_slice)\n",
        "                            dA_prev[i, h_start:h_end, w_start:w_end, c] += np.multiply(mask, dZ[i, h, w, c])\n",
        "                        elif self.mode == \"average\":\n",
        "                            dz = dZ[i, h, w, c]\n",
        "                            dA_prev[i, h_start:h_end, w_start:w_end, c] += self.distribute_value(dz, self.kernel_size)\n",
        "                        else:\n",
        "                            raise ValueError(\"Invalid mode\")\n",
        "        # Don't change the return\n",
        "        return dA_prev, None\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "MhKaw-POaOiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The `BinaryCrossEntropy` class represents the binary cross entropy loss function. It has two main methods:\n",
        "\n",
        "1. `compute(self, y_hat: np.ndarray, y: np.ndarray) -> float`: This method computes the binary cross entropy loss. It takes the predicted labels `y_hat` and the true labels `y` as input. The method calculates the loss using the formula `-y * log(y_hat) - (1 - y) * log(1 - y_hat)` and returns the loss value as a float.\n",
        "\n",
        "2. `backward(self, y_hat: np.ndarray, y: np.ndarray) -> np.ndarray`: This method computes the derivative of the binary cross entropy loss with respect to the predicted labels `y_hat`. It takes `y_hat` and `y` as input and returns the derivative of the loss as a numpy array. The derivative is calculated using the formula `-(y / y_hat) + ((1 - y) / (1 - y_hat))`, which is derived from the chain rule.\n"
      ],
      "metadata": {
        "id": "FcLU01WE-YYG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class BinaryCrossEntropy:\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def compute(self, y_hat: np.ndarray, y: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Computes the binary cross entropy loss.\n",
        "            args:\n",
        "                y: true labels (n_classes, batch_size)\n",
        "                y_hat: predicted labels (n_classes, batch_size)\n",
        "            returns:\n",
        "                binary cross entropy loss\n",
        "        \"\"\"\n",
        "        # TODO: Implement binary cross entropy loss\n",
        "        batch_size = y.shape[1]\n",
        "        cost = np.log(y_hat - y)\n",
        "        return np.squeeze(cost)\n",
        "\n",
        "    def backward(self, y_hat: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes the derivative of the binary cross entropy loss.\n",
        "            args:\n",
        "                y: true labels (n_classes, batch_size)\n",
        "                y_hat: predicted labels (n_classes, batch_size)\n",
        "            returns:\n",
        "                derivative of the binary cross entropy loss\n",
        "        \"\"\"\n",
        "        # hint: use the np.divide function\n",
        "        # TODO: Implement backward pass for binary cross entropy loss\n",
        "        return -(y / y_hat) + ((1 - y) / (1 - y_hat))\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Tp3D4bEPaOiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The `MeanSquaredError` class represents the mean squared error loss function. It has two main methods:\n",
        "\n",
        "1. `compute(self, y_pred, y_true)`: This method computes the mean squared error (MSE) loss. It takes the predicted labels `y_pred` and the true labels `y_true` as input. The method calculates the loss using the formula `mean((y_pred - y_true)^2)` and returns the loss value.\n",
        "\n",
        "2. `backward(self, y_pred, y_true)`: This method computes the derivative of the mean squared error loss with respect to the predicted labels `y_pred`. It takes `y_pred` and `y_true` as input and returns the derivative of the loss. The derivative is calculated using the formula `(y_pred - y_true) / batch_size`, where `batch_size` is the number of examples in the batch."
      ],
      "metadata": {
        "id": "_VUDLzwJ-T-l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "class MeanSquaredError:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def compute(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        computes the mean squared error loss\n",
        "            args:\n",
        "                y_pred: predicted labels (n_classes, batch_size)\n",
        "                y_true: true labels (n_classes, batch_size)\n",
        "            returns:\n",
        "                mean squared error loss\n",
        "        \"\"\"\n",
        "        # TODO: Implement mean squared error loss\n",
        "        batch_size = y_pred.shape[1]\n",
        "        cost = np.sum(np.square(y_pred - y_true)) / (2 * batch_size)\n",
        "        return np.squeeze(cost)\n",
        "\n",
        "    def backward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        computes the derivative of the mean squared error loss\n",
        "            args:\n",
        "                y_pred: predicted labels (n_classes, batch_size)\n",
        "                y_true: true labels (n_classes, batch_size)\n",
        "            returns:\n",
        "                derivative of the mean squared error loss\n",
        "        \"\"\"\n",
        "        # TODO: Implement backward pass for mean squared error loss\n",
        "        return (y_pred - y_true) / y_pred.shape[1]"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "j-Va5MeqaOiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The `GD` class represents the Gradient Descent optimizer. It is used to update the parameters of the layers in a neural network based on the gradients computed during backpropagation. It has three main components:\n",
        "\n",
        "1. `__init__(self, layers_list: dict, learning_rate: float)`: This method initializes the Gradient Descent optimizer. It takes a dictionary `layers_list` containing the names and layer objects of the network's layers, and a `learning_rate` which determines the step size for parameter updates.\n",
        "\n",
        "2. `update(self, grads, name)`: This method performs the parameter update for a specific layer. It takes a list of gradients `grads` for the weights and biases of the layer, and the `name` of the layer. It returns a list of updated parameters.\n",
        "\n",
        "   Inside the method, the corresponding layer object is obtained from `self.layers` using the provided `name`. Then, for each gradient in `grads`, the corresponding parameter is updated using the gradient descent update rule: `new_param = old_param - learning_rate * gradient`. The updated parameters are stored in a list `params`, which is then returned."
      ],
      "metadata": {
        "id": "cdRDlgJX-QsF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Implement the gradient descent optimizer\n",
        "class GD:\n",
        "    def __init__(self, layers_list: dict, learning_rate: float):\n",
        "        \"\"\"\n",
        "        Gradient Descent optimizer.\n",
        "            args:\n",
        "                layers_list: dictionary of layers name and layer object\n",
        "                learning_rate: learning rate\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.layers = layers_list\n",
        "\n",
        "    def update(self, grads, name):\n",
        "        \"\"\"\n",
        "        Update the parameters of the layer.\n",
        "            args:\n",
        "                grads: list of gradients for the weights and bias\n",
        "                name: name of the layer\n",
        "            returns:\n",
        "                params: list of updated parameters\n",
        "        \"\"\"\n",
        "        layer = self.layers[name]\n",
        "        params = []\n",
        "        #TODO: Implement gradient descent update\n",
        "        for i in range(len(grads)):\n",
        "            params.append(layer.parameters[i] - self.learning_rate * grads[i])\n",
        "        return params"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "WUHzwzJ1aOiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The `Adam` class represents the Adam optimizer, an extension of gradient descent optimization algorithm that adapts the learning rate for each parameter in the network. It has the following components:\n",
        "\n",
        "1. `__init__(self, layers_list, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8)`: This method initializes the Adam optimizer. It takes a dictionary `layers_list` containing the names and layer objects of the network's layers, and optional parameters `learning_rate`, `beta1`, `beta2`, and `epsilon` that control the behavior of the optimizer.\n",
        "\n",
        "   Inside the method, the initial values for the parameters of Adam optimizer are set. `self.V` and `self.S` are dictionaries that store the first and second moments of the gradients, respectively. For each layer in `layers_list`, the initial `V` and `S` values are created as lists of zeros with the same shape as the parameters of the layer. These `V` and `S` lists are stored in the dictionaries `self.V` and `self.S` using the layer index as the key.\n",
        "\n",
        "2. `update(self, grads, name, epoch)`: This method performs the parameter update for a specific layer using the Adam optimizer. It takes a list of gradients `grads`, the `name` of the layer, and the current `epoch` number. It returns a list of updated parameters.\n",
        "\n",
        "   Inside the method, the corresponding layer object is obtained from `self.layers` using the provided `name`. For each gradient in `grads`, the `V` and `S` values are updated using the Adam update equations. The updated parameters are computed using the Adam update rule: `new_param = old_param - learning_rate * V / (sqrt(S) + epsilon)`. The updated parameters are stored in a list `params`, which is then returned."
      ],
      "metadata": {
        "id": "qk2gj1Ru-L-l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Implement Adam optimizer\n",
        "class Adam:\n",
        "    def __init__(self, layers_list, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.layers = layers_list\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.V = {}\n",
        "        self.S = {}\n",
        "        for i in range(len(layers_list)):\n",
        "            # TODO: Initialize V and S for each layer (v and s are lists of zeros with the same shape as the parameters)\n",
        "            v = [np.zeros_like(p) for p in layers_list[i].parameters]\n",
        "            s = [np.zeros_like(p) for p in layers_list[i].parameters]\n",
        "            self.V[i] = v\n",
        "            self.S[i] = s\n",
        "\n",
        "    def update(self, grads, name, epoch):\n",
        "        layer = self.layers[name]\n",
        "        params = []\n",
        "        # TODO: Implement Adam update\n",
        "        for i in range(len(grads)):\n",
        "            self.V[name][i] = self.beta1 * self.V[name][i] + (1 - self.beta1) * grads[i]\n",
        "            self.S[name][i] = self.beta2 * self.S[name][i]  +(1 - self.beta2) * np.square(grads[i])\n",
        "            self.V[name][i] /= (1 - np.power(self.beta1, epoch+1)) # TODO: correct V\n",
        "            self.S[name][i] /= (1 - np.power(self.beta2, epoch+1)) # TODO: correct S\n",
        "            params.append(layer.parameters[i] - self.learing_rate * self.V[name][i]/ (np.sqrt(self.S[name][i]) + self.epsilon))\n",
        "        return params"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "5hl6AbdhaOiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code defines a hierarchy of activation functions for use in neural networks. It consists of an abstract base class `Activation` and several concrete activation classes (`Sigmoid`, `ReLU`, `Tanh`, `LinearActivation`).\n",
        "\n",
        "The `Activation` class is an abstract base class that defines the interface for activation functions. It has two abstract methods: `forward(self, Z: np.ndarray) -> np.ndarray` and `backward(self, dA: np.ndarray, Z: np.ndarray) -> np.ndarray`. These methods represent the forward pass and backward pass of the activation function, respectively. Any concrete activation class must implement these methods.\n",
        "\n",
        "The concrete activation classes (`Sigmoid`, `ReLU`, `Tanh`, `LinearActivation`) inherit from the `Activation` class and provide the implementations for the forward and backward pass of each specific activation function.\n",
        "\n",
        "The `get_activation` function is a helper function that returns the activation function and its derivative based on the given activation name. It takes an `activation` parameter, which is a string representing the name of the desired activation function. It returns a tuple containing the activation class and its derivative.\n",
        "\n",
        "Let's go through each activation function briefly:\n",
        "\n",
        "- `Sigmoid`: This class implements the sigmoid activation function. The `forward` method computes the sigmoid function `1 / (1 + exp(-Z))`, and the `backward` method computes the derivative of the cost with respect to `Z` as `dA * A * (1 - A)`, where `A` is the output of the sigmoid function.\n",
        "\n",
        "- `ReLU`: This class implements the rectified linear unit (ReLU) activation function. The `forward` method computes the ReLU function `max(0, Z)`, and the `backward` method computes the derivative of the cost with respect to `Z` as `dA` where `Z > 0`, and 0 otherwise.\n",
        "\n",
        "- `Tanh`: This class implements the hyperbolic tangent (tanh) activation function. The `forward` method computes the tanh function `tanh(Z)`, and the `backward` method computes the derivative of the cost with respect to `Z` as `dA * (1 - tanh(Z)^2)`.\n",
        "\n",
        "- `LinearActivation`: This class represents the linear activation function. The `forward` method simply returns `Z` as the output, and the `backward` method computes the derivative of the cost with respect to `Z` as `dA * 1`.\n"
      ],
      "metadata": {
        "id": "fZ2t4hqV-HiN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Activation:\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Forward pass for activation function.\n",
        "            args:\n",
        "                Z: input to the activation function\n",
        "            returns:\n",
        "                A: output of the activation function\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, dA: np.ndarray, Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Backward pass for activation function.\n",
        "            args:\n",
        "                dA: derivative of the cost with respect to the activation\n",
        "                Z: input to the activation function\n",
        "            returns:\n",
        "                derivative of the cost with respect to Z\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class Sigmoid(Activation):\n",
        "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Sigmoid activation function.\n",
        "            args:\n",
        "            x: input to the activation function\n",
        "            returns:\n",
        "                sigmoid(x)\n",
        "        \"\"\"\n",
        "        # TODO: Implement sigmoid activation function\n",
        "        A = 1. / 1. + np.exp(-Z)\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA: np.ndarray, Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Backward pass for sigmoid activation function.\n",
        "            args:\n",
        "                dA: derivative of the cost with respect to the activation\n",
        "                Z: input to the activation function\n",
        "            returns:\n",
        "                derivative of the cost with respect to Z\n",
        "        \"\"\"\n",
        "        A = self.forward(Z)\n",
        "        # TODO: Implement backward pass for sigmoid activation function\n",
        "        dZ = dA * A * (1 - A)\n",
        "        return dZ\n",
        "\n",
        "\n",
        "class ReLU(Activation):\n",
        "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        ReLU activation function.\n",
        "            args:\n",
        "                x: input to the activation function\n",
        "            returns:\n",
        "                relu(x)\n",
        "        \"\"\"\n",
        "        # TODO: Implement ReLU activation function\n",
        "        A = np.maximum(0, Z)\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA: np.ndarray, Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Backward pass for ReLU activation function.\n",
        "            args:\n",
        "                dA: derivative of the cost with respect to the activation\n",
        "                Z: input to the activation function\n",
        "            returns:\n",
        "                derivative of the cost with respect to Z\n",
        "        \"\"\"\n",
        "        # TODO: Implement backward pass for ReLU activation function\n",
        "        dZ = np.array(dA, copy=True)\n",
        "        dZ[Z <= 0] = 0\n",
        "\n",
        "        return dZ\n",
        "\n",
        "\n",
        "\n",
        "class Tanh(Activation):\n",
        "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Tanh activation function.\n",
        "            args:\n",
        "                x: input to the activation function\n",
        "            returns:\n",
        "                tanh(x)\n",
        "        \"\"\"\n",
        "        # TODO: Implement tanh activation function\n",
        "        A = np.tanh(Z)\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA: np.ndarray, Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Backward pass for tanh activation function.\n",
        "            args:\n",
        "                dA: derivative of the cost with respect to the activation\n",
        "                Z: input to the activation function\n",
        "            returns:\n",
        "                derivative of the cost with respect to Z\n",
        "        \"\"\"\n",
        "        A = self.forward(Z)\n",
        "        # TODO: Implement backward pass for tanh activation function\n",
        "        dZ = dA * (1 - np.square(np.tanh(Z)))\n",
        "        return dZ\n",
        "\n",
        "class LinearActivation(Activation):\n",
        "    def linear(Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Linear activation function.\n",
        "            args:\n",
        "                x: input to the activation function\n",
        "            returns:\n",
        "                x\n",
        "        \"\"\"\n",
        "        # TODO: Implement linear activation function\n",
        "        A = Z\n",
        "        return A\n",
        "\n",
        "    def backward(dA: np.ndarray, Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Backward pass for linear activation function.\n",
        "            args:\n",
        "                dA: derivative of the cost with respect to the activation\n",
        "                Z: input to the activation function\n",
        "            returns:\n",
        "                derivative of the cost with respect to Z\n",
        "        \"\"\"\n",
        "        # TODO: Implement backward pass for linear activation function\n",
        "        dZ = dA * np.ones_like(Z)\n",
        "        return dZ\n",
        "\n",
        "def get_activation(activation: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Returns the activation function and its derivative.\n",
        "        args:\n",
        "            activation: activation function name\n",
        "        returns:\n",
        "            activation function and its derivative\n",
        "    \"\"\"\n",
        "    if activation == 'sigmoid':\n",
        "        return Sigmoid\n",
        "    elif activation == 'relu':\n",
        "        return ReLU\n",
        "    elif activation == 'tanh':\n",
        "        return Tanh\n",
        "    elif activation == 'linear':\n",
        "        return LinearActivation\n",
        "    else:\n",
        "        raise ValueError('Activation function not supported')"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "IlMTVqtlaOiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- `__init__(self, arch, criterion, optimizer, name=None)`: Initializes the model with the given architecture, loss criterion, optimizer, and an optional name.\n",
        "- `is_layer(self, layer)`: Checks if the given layer is a valid layer.\n",
        "- `is_activation(self, layer)`: Checks if the given layer is an activation function.\n",
        "- `forward(self, x)`: Performs a forward pass through the model and returns the output.\n",
        "- `backward(self, dAL, tmp, x)`: Performs a backward pass through the model and computes the gradients.\n",
        "- `update(self, grads)`: Updates the model using the computed gradients.\n",
        "- `one_epoch(self, x, y)`: Performs one epoch of training on the given input and labels.\n",
        "- `save(self, name)`: Saves the model to a file.\n",
        "- `load_model(self, name)`: Loads a saved model from a file.\n",
        "- `shuffle(self, m, shuffling)`: Shuffles the order of the data.\n",
        "- `batch(self, X, y, batch_size, index, order)`: Retrieves a batch of data from the input and labels.\n",
        "- `compute_loss(self, X, y, batch_size)`: Computes the loss for the given input and labels.\n",
        "- `train(self, X, y, epochs, val=None, batch_size=3, shuffling=False, verbose=1, save_after=None)`: Trains the model for the specified number of epochs using the given input and labels.\n",
        "- `predict(self, X)`: Performs prediction on the given input and returns the output.\n"
      ],
      "metadata": {
        "id": "yg0Y--bM9_mN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self, arch, criterion, optimizer, name=None):\n",
        "        \"\"\"\n",
        "        Initialize the model.\n",
        "        args:\n",
        "            arch: dictionary containing the architecture of the model\n",
        "            criterion: loss\n",
        "            optimizer: optimizer\n",
        "            name: name of the model\n",
        "        \"\"\"\n",
        "        if name is None:\n",
        "            self.model = arch\n",
        "            self.criterion = criterion\n",
        "            self.optimizer = optimizer\n",
        "            self.layers_names = list(arch.keys())\n",
        "        else:\n",
        "            self.model, self.criterion, self.optimizer, self.layers_names = self.load_model(name)\n",
        "\n",
        "    def is_layer(self, layer):\n",
        "        \"\"\"\n",
        "        Check if the layer is a layer.\n",
        "        args:\n",
        "            layer: layer to be checked\n",
        "        returns:\n",
        "            True if the layer is a layer, False otherwise\n",
        "        \"\"\"\n",
        "        # TODO: Implement check if the layer is a layer\n",
        "        return isinstance(layer, FC) or isinstance(layer, Conv2D) or isinstance(layer, MaxPool2D)\n",
        "\n",
        "    def is_activation(self, layer):\n",
        "        \"\"\"\n",
        "        Check if the layer is an activation function.\n",
        "        args:\n",
        "            layer: layer to be checked\n",
        "        returns:\n",
        "            True if the layer is an activation function, False otherwise\n",
        "        \"\"\"\n",
        "        # TODO: Implement check if the layer is an activation\n",
        "        return isinstance(layer, Activation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "        args:\n",
        "            x: input to the model\n",
        "        returns:\n",
        "            output of the model\n",
        "        \"\"\"\n",
        "        tmp = []\n",
        "        A = x\n",
        "        # TODO: Implement forward pass through the model\n",
        "        # NOTICE: we have a pattern of layers and activations\n",
        "        for l in range(len(self.layer_names), 2):\n",
        "            Z = self.model[self.layer_names[l]].forward(A)\n",
        "            tmp.append(np.copy(Z))    # hint add a copy of Z to tmp\n",
        "            A = self.model[self.layer_names[l+1]].forward(Z)\n",
        "            tmp.append(np.copy(A))    # hint add a copy of A to tmp\n",
        "        return tmp\n",
        "\n",
        "    def backward(self, dAL, tmp, x):\n",
        "        \"\"\"\n",
        "        Backward pass through the model.\n",
        "        args:\n",
        "            dAL: derivative of the cost with respect to the output of the model\n",
        "            tmp: list containing the intermediate values of Z and A\n",
        "            x: input to the model\n",
        "        returns:\n",
        "            gradients of the model\n",
        "        \"\"\"\n",
        "        dA = dAL\n",
        "        grads = {}\n",
        "        # TODO: Implement backward pass through the model\n",
        "        # NOTICE: we have a pattern of layers and activations\n",
        "        # for from the end to the beginning of the tmp list\n",
        "        for l in range(len(layer_names)):\n",
        "            if l > 2:\n",
        "                Z, A = tmp[l - 1], tmp[l - 2]\n",
        "            else:\n",
        "                Z, A = tmp[l - 1], x\n",
        "            dZ = self.model[self.layer_names[l]].backward(dA, Z)\n",
        "            dA, grad = self.model[self.layer_names[l - 1]].backward(dZ, A)\n",
        "            grads[self.layers_names[l - 1]] = grad\n",
        "        return grads\n",
        "\n",
        "    def update(self, grads):\n",
        "        \"\"\"\n",
        "        Update the model.\n",
        "        args:\n",
        "            grads: gradients of the model\n",
        "        \"\"\"\n",
        "        for n in self.layer_names:\n",
        "            if self.is_layer(self.model[n]) and not (isinstance(self.model[n] ,MaxPool2D)) :    # hint check if the layer is a layer and also is not a maxpooling layer\n",
        "                self.model[n].update(self.optimizer, grads[n])\n",
        "\n",
        "    def one_epoch(self, x, y):\n",
        "        \"\"\"\n",
        "        One epoch of training.\n",
        "        args:\n",
        "            x: input to the model\n",
        "            y: labels\n",
        "            batch_size: batch size\n",
        "        returns:\n",
        "            loss\n",
        "        \"\"\"\n",
        "        # TODO: Implement one epoch of training\n",
        "        tmp = self.forward(x)\n",
        "        AL = tmp[-1]\n",
        "        loss = self.criterion.compute(AL, y)\n",
        "        dAL = self.criterion.backward(AL, y)\n",
        "        grads =  self.backward(dAL, tmp, x)\n",
        "        self.update(grads)\n",
        "        return loss\n",
        "\n",
        "    def save(self, name):\n",
        "        \"\"\"\n",
        "        Save the model.\n",
        "        args:\n",
        "            name: name of the model\n",
        "        \"\"\"\n",
        "        with open(name, 'wb') as f:\n",
        "            pickle.dump((self.model, self.criterion, self.optimizer, self.layers_names), f)\n",
        "\n",
        "    def load_model(self, name):\n",
        "        \"\"\"\n",
        "        Load the model.\n",
        "        args:\n",
        "            name: name of the model\n",
        "        returns:\n",
        "            model, criterion, optimizer, layers_names\n",
        "        \"\"\"\n",
        "        with open(name, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    def shuffle(self, m, shuffling):\n",
        "        order = list(range(m))\n",
        "        if shuffling:\n",
        "            return np.random.shuffle(order)\n",
        "        return order\n",
        "\n",
        "    def batch(self, X, y, batch_size, index, order):\n",
        "        \"\"\"\n",
        "        Get a batch of data.\n",
        "        args:\n",
        "            X: input to the model\n",
        "            y: labels\n",
        "            batch_size: batch size\n",
        "            index: index of the batch\n",
        "                e.g: if batch_size = 3 and index = 1 then the batch will be from index [3, 4, 5]\n",
        "            order: order of the data\n",
        "        returns:\n",
        "            bx, by: batch of data\n",
        "        \"\"\"\n",
        "        # TODO: Implement batch\n",
        "        last_index = min(((index + 1) * batch_size),\n",
        "                         len(order))  # hint last index of the batch check for the last batch\n",
        "        batch = order[(index * batch_size): last_index]\n",
        "        # NOTICE: inputs are 4 dimensional or 2 demensional\n",
        "        if len(X.shape) == 2:\n",
        "            bx = X[:, batch]\n",
        "            by = y[:, batch]\n",
        "            return bx, by\n",
        "        else:\n",
        "            bx = X[batch]\n",
        "            by = y[batch]\n",
        "            return bx, by\n",
        "\n",
        "\n",
        "    def compute_loss(self, X, y, batch_size):\n",
        "        \"\"\"\n",
        "        Compute the loss.\n",
        "        args:\n",
        "            X: input to the model\n",
        "            y: labels\n",
        "            Batch_Size: batch size\n",
        "        returns:\n",
        "            loss\n",
        "        \"\"\"\n",
        "        # TODO: Implement compute loss\n",
        "        m = X.shape[0] if len(X.shape) == 4 else X.shape[1]\n",
        "        order = self.shuffle(m, False)\n",
        "        cost = 0\n",
        "        for b in range(m // batch_size):\n",
        "            bx, by = self.batch(X, y, batch_size, b, order)\n",
        "            tmp = self.forward(bx)\n",
        "            AL = tmp[-1]\n",
        "            cost += self.criterion.compute(AL, y)(m // batch_size)\n",
        "        return cost\n",
        "\n",
        "\n",
        "    def train(self, X, y, epochs, val=None, batch_size=3, shuffling=False, verbose=1, save_after=None):\n",
        "        \"\"\"\n",
        "        Train the model.\n",
        "        args:\n",
        "            X: input to the model\n",
        "            y: labels\n",
        "            epochs: number of epochs\n",
        "            val: validation data\n",
        "            batch_size: batch size\n",
        "            shuffling: if True shuffle the data\n",
        "            verbose: if 1 print the loss after each epoch\n",
        "            save_after: save the model after training\n",
        "        \"\"\"\n",
        "        # TODO: Implement training\n",
        "        train_cost = []\n",
        "        val_cost = []\n",
        "        # NOTICE: if your inputs are 4 dimensional m = X.shape[0] else m = X.shape[1]\n",
        "        m = X.shape[0] if len(X.shape) == 4 else X.shape[1]\n",
        "        for e in tqdm(range(1, epochs + 1)):\n",
        "            order = self.shuffle(m, shuffling)\n",
        "            cost = 0\n",
        "            for b in range(m // batch_size):\n",
        "                bx, by = self.batch(X, y, batch_size, b, order)\n",
        "                cost += (self.one_epoch(X,y)) / (m // batch_size)\n",
        "            train_cost.append(cost)\n",
        "            if val is not None:\n",
        "                val_cost.append(self.compute_loss(val, y, batch_size))\n",
        "            if verbose != False:\n",
        "                if e % verbose == 0:\n",
        "                    print(\"Epoch {}: train cost = {}\".format(e, cost))\n",
        "                if val is not None:\n",
        "                    print(\"Epoch {}: val cost = {}\".format(e, val_cost[-1]))\n",
        "        if save_after is not None:\n",
        "            self.save(save_after)\n",
        "        return train_cost, val_cost\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the output of the model.\n",
        "        args:\n",
        "            X: input to the model\n",
        "        returns:\n",
        "            predictions\n",
        "        \"\"\"\n",
        "        # TODO: Implement prediction\n",
        "        return self.forward(X)[-1]\n",
        ""
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KU4y8VfeaOiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. The `construct_dataset_mnist` function:\n",
        "   - It starts by retrieving the list of files in the directory specified by `mnist_path[2]` (presumably containing MNIST digit 2 images) using `os.listdir`.\n",
        "   - Then, it creates a list `MNIST2FILES` by appending the directory path to each file name.\n",
        "   - Similarly, it retrieves the list of files in the directory specified by `mnist_path[5]` (presumably containing MNIST digit 5 images).\n",
        "   - It creates a list `MNIST5FILES` by appending the directory path to each file name.\n",
        "   - The function initializes two empty lists, `X` and `y`.\n",
        "   - It then loops through each file path in `MNIST2FILES` and performs the following:\n",
        "     - It opens the image using `Image.open` and converts it to a NumPy array using `np.array`.\n",
        "     - The pixel values are divided by 255.0 to scale them between 0 and 1.\n",
        "     - `np.expand_dims` is used to add an extra dimension to the array, making it compatible with convolutional layers (axis=-1 represents the last dimension).\n",
        "     - The array is appended to `X`, and the label 0 is appended to `y`.\n",
        "   - The same process is repeated for each file path in `MNIST5FILES`, but the label is set to 1.\n",
        "   - Finally, the function returns the lists `X` and `y`.\n",
        "\n",
        "2. Reading the CSV file using Pandas:\n",
        "   - The `pd.read_csv` function is used to read the CSV file specified by `cali_path['train']` into a DataFrame `df`.\n",
        "\n",
        "3. Preparing the input features `X` and labels `y`:\n",
        "   - `X` is created by selecting specific columns from the DataFrame `df` using `df[['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']]`.\n",
        "   - `y` is created by selecting the 'median_house_value' column from the DataFrame `df`.\n",
        "   - Both `X` and `y` are converted to NumPy arrays using `np.array\n",
        "\n",
        "`.\n",
        "\n",
        "4. Z-score scaling the input features `X`:\n",
        "   - The `zscore_scaling` function is defined, which takes an input array `data`.\n",
        "   - It calculates the mean and standard deviation of the data using `np.mean` and `np.std`.\n",
        "   - The scaled data is obtained by subtracting the mean and dividing by the standard deviation: `(data - mean) / std`.\n",
        "   - The scaled data is returned.\n",
        "   - The `zscore_scaling` function is applied to `X` using `Xt = zscore_scaling(X)`.\n",
        "\n",
        "5. The labels `y` are already in NumPy array format and not scaled.\n",
        "\n",
        "The resulting scaled input features are stored in `Xt`, and the labels are stored in `yt`."
      ],
      "metadata": {
        "id": "HbEei27f9yn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_dataset_mnist():\n",
        "  directory = os.listdir(mnist_path[2])\n",
        "  MNIST2FILES = [mnist_path[2] + file_name for file_name in directory]\n",
        "  directory = os.listdir(mnist_path[5])\n",
        "  MNIST5FILES = [mnist_path[5] + file_name for file_name in directory]\n",
        "\n",
        "  X, y  = [], []\n",
        "  for image_path in MNIST2FILES:\n",
        "    X.append(np.expand_dims(np.array(Image.open(image_path)) / 255., axis=-1))\n",
        "    y.append(0)\n",
        "  for image_path in MNIST5FILES:\n",
        "    X.append(np.expand_dims(np.array(Image.open(image_path)) / 255., axis=-1))\n",
        "    y.append(1)\n",
        "\n",
        "  return X, y\n",
        "\n",
        "X_MNIST, y_MNIST = construct_dataset_mnist()\n",
        "\n",
        "df = pd.read_csv(cali_path['train'])\n",
        "\n",
        "X = df[['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']]\n",
        "y = df['median_house_value']\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "def zscore_scaling(data):\n",
        "    mean = np.mean(data)\n",
        "    std = np.std(data)\n",
        "    scaled_data = (data - mean) / std\n",
        "    return scaled_data\n",
        "\n",
        "Xt = zscore_scaling(X)\n",
        "yt = y\n"
      ],
      "metadata": {
        "id": "Q5By6q9K9uv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code defines an architecture for the model, `arch_model_MNIST`, using a dictionary. The architecture consists of several layers and activation functions. Here's a breakdown of the architecture:\n",
        "\n",
        "- `\"C1\"`: A `Conv2D` layer with 1 input channel, 2 output channels, a kernel size of (10, 10), stride of (1, 1), and padding of (1, 1).\n",
        "- `\"relu1\"`: A ReLU activation function.\n",
        "- `\"C2\"`: Another `Conv2D` layer with 2 input channels, 4 output channels, a kernel size of (5, 5), stride of (1, 1), and no padding.\n",
        "- `\"relu2\"`: Another ReLU activation function.\n",
        "- `\"DENSE1\"`: A fully connected (`FC`) layer with 256 input units and 16 output units.\n",
        "- `\"s1\"`: A sigmoid activation function.\n",
        "- `\"DENSE2\"`: Another `FC` layer with 16 input units and 1 output unit.\n",
        "- `\"s2\"`: Another sigmoid activation function.\n",
        "\n",
        "In this part, the code creates an instance of the `BinaryCrossEntropy` loss criterion and an instance of the `Adam` optimizer. The `Adam` optimizer is initialized with the `arch_model_MNIST` architecture and a learning rate of 0.01. Finally, the `Model` class is instantiated with the `arch_model_MNIST`, `criterion`, and `optimizer`, resulting in the `myModel` object.\n",
        "\n",
        "The architecture (`arch_model_california_pricing`) consists of the following layers:\n",
        "\n",
        "1. `\"DENSE1\"`: Fully Connected (FC) layer with 8 input units and 16 output units.\n",
        "2. `\"relu1\"`: ReLU activation function.\n",
        "3. `\"DENSE2\"`: FC layer with 16 input units and 16 output units.\n",
        "4. `\"relu2\"`: ReLU activation function.\n",
        "5. `\"DENSE3\"`: FC layer with 16 input units and 1 output unit.\n",
        "6. `'s1'`: Sigmoid activation function.\n",
        "\n",
        "The loss criterion (`criterion2`) is Mean Squared Error (MSE), which is commonly used for regression tasks.\n",
        "\n",
        "The optimizer (`optimizer2`) is Adam, an optimization algorithm that performs adaptive learning rate updates.\n",
        "\n",
        "This creates an instance of the `Model` class, which can be used for training, evaluation, and prediction on the California pricing task."
      ],
      "metadata": {
        "id": "FgLdq185_Rau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arch_model_MNIST = {\n",
        "    \"C1\": Conv2D(1, 2, name=\"CONV1\", kernel_size=(10, 10), stride=(1, 1), padding=(1, 1)),\n",
        "    \"relu1\": get_activation(\"relu\")(),\n",
        "    \"C2\": Conv2D(2, 4, name=\"CONV2\", kernel_size=(5, 5), stride=(1, 1), padding=(0, 0)),\n",
        "    \"relu2\": get_activation(\"relu\")(),\n",
        "    \"DENSE1\": FC(256, 16, \"FC1\"),\n",
        "    \"s1\": get_activation(\"sigmoid\")(),\n",
        "    \"DENSE2\": FC(16, 1, \"FC2\"),\n",
        "    \"s2\": get_activation(\"sigmoid\")(),\n",
        "}\n",
        "criterion_1 = BinaryCrossEntropy()\n",
        "optimizer_1 = Adam(arch_model_MNIST, learning_rate=0.01)\n",
        "\n",
        "model_MNIST = Model(arch_model_MNIST, criterion_1, optimizer_1)\n",
        "\n",
        "arch_model_california_pricing = {\n",
        "    \"DENSE1\": FC(8, 16, 'DENSE1'),\n",
        "    \"relu1\": get_activation(\"relu\")(),\n",
        "    \"DENSE2\": FC(16, 16, 'DENSE2'),\n",
        "    \"relu2\": get_activation(\"relu\")(),\n",
        "    \"DENSE3\": FC(16, 1, 'DENSE3'),\n",
        "    's1': get_activation(\"sigmoid\")(),\n",
        "}\n",
        "criterion_2 = MeanSquaredError()\n",
        "optimizer_2 = Adam(arch_model_california_pricing, learning_rate=0.01)\n",
        "\n",
        "CHP_Model = Model(arch_model_california_pricing, criterion_2, optimizer_2)\n",
        "\n"
      ],
      "metadata": {
        "id": "qN5kxRk3_RKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_MNIST.train(X_MNIST, y_MNIST, 10, shuffling=True, save_after='/content/drive/MyDrive/MODELS/MNIST')"
      ],
      "metadata": {
        "id": "AqF_1qTU_naW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHP_Model.trian(Xt, yt, 10, save_after='/content/drive/MyDrive/MODELS/CALIFORNIA_HOUSE_PRICING')"
      ],
      "metadata": {
        "id": "YzbSEtRxBdlG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}