{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from abc import ABC, abstractmethod\n",
        "import pickle\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from random import shuffle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "eNMtuu2YSo24",
        "outputId": "d0c9fc74-2f66-4414-9ce5-8fb778725441",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class FC:\n",
        "    def __init__(self, input_size : int, output_size : int, name : str, initialize_method : str=\"random\"):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.name = name\n",
        "        self.initialize_method = initialize_method\n",
        "        self.parameters = [self.initialize_weights(), self.initialize_bias()]\n",
        "        self.input_shape = None\n",
        "        self.reshaped_shape = None\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        if self.initialize_method == \"random\":\n",
        "            # TODO: Initialize weights with random values using np.random.randn\n",
        "            return None * 0.01\n",
        "\n",
        "        elif self.initialize_method == \"xavier\":\n",
        "            return None\n",
        "\n",
        "        elif self.initialize_method == \"he\":\n",
        "            return None\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid initialization method\")\n",
        "\n",
        "    def initialize_bias(self):\n",
        "        # TODO: Initialize bias with zeros\n",
        "        return np.zeros((None, 1))\n",
        "\n",
        "    def forward(self, A_prev):\n",
        "        \"\"\"\n",
        "        Forward pass for fully connected layer.\n",
        "            args:\n",
        "                A_prev: activations from previous layer (or input data)\n",
        "                A_prev.shape = (batch_size, input_size)\n",
        "            returns:\n",
        "                Z: output of the fully connected layer\n",
        "        \"\"\"\n",
        "        # NOTICE: BATCH_SIZE is the first dimension of A_prev\n",
        "        self.input_shape = A_prev.shape\n",
        "        A_prev_tmp = np.copy(A_prev)\n",
        "\n",
        "        # TODO: Implement forward pass for fully connected layer\n",
        "        if None: # check if A_prev is output of convolutional layer\n",
        "            batch_size = None\n",
        "            A_prev_tmp = A_prev_tmp.reshape(None, -1).T\n",
        "        self.reshaped_shape = A_prev_tmp.shape\n",
        "\n",
        "        # TODO: Forward part\n",
        "        W, b = None\n",
        "        Z = None @ None + None\n",
        "        return Z\n",
        "\n",
        "    def backward(self, dZ, A_prev):\n",
        "        \"\"\"\n",
        "        Backward pass for fully connected layer.\n",
        "            args:\n",
        "                dZ: derivative of the cost with respect to the output of the current layer\n",
        "                A_prev: activations from previous layer (or input data)\n",
        "            returns:\n",
        "                dA_prev: derivative of the cost with respect to the activation of the previous layer\n",
        "                grads: list of gradients for the weights and bias\n",
        "        \"\"\"\n",
        "        A_prev_tmp = np.copy(A_prev)\n",
        "        if None: # check if A_prev is output of convolutional layer\n",
        "            batch_size = None\n",
        "            A_prev_tmp = A_prev_tmp.reshape(None, -1).T\n",
        "\n",
        "        # TODO: backward part\n",
        "        W, b = None\n",
        "        dW = None @ None.T / None\n",
        "        db = np.sum(None, axis=1, keepdims=True) / None\n",
        "        dA_prev = None.T @ None\n",
        "        grads = [dW, db]\n",
        "        # reshape dA_prev to the shape of A_prev\n",
        "        if None:    # check if A_prev is output of convolutional layer\n",
        "            dA_prev = dA_prev.T.reshape(self.input_shape)\n",
        "        return dA_prev, grads\n",
        "\n",
        "    def update_parameters(self, optimizer, grads):\n",
        "        \"\"\"\n",
        "        Update the parameters of the layer.\n",
        "            args:\n",
        "                optimizer: optimizer object\n",
        "                grads: list of gradients for the weights and bias\n",
        "        \"\"\"\n",
        "        self.parameters = optimizer.update(grads, self.name)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "NIBzSJGVSo26"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class Conv2D:\n",
        "    def __init__(self, in_channels, out_channels, name, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), initialize_method=\"random\"):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.name = name\n",
        "        self.initialize_method = initialize_method\n",
        "\n",
        "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
        "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
        "        self.padding = (padding, padding) if isinstance(padding, int) else padding\n",
        "        self.parameters = [self.initialize_weights(), self.initialize_bias()]\n",
        "\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize weights.\n",
        "        returns:\n",
        "            weights: initialized kernel with shape: (kernel_size[0], kernel_size[1], in_channels, out_channels)\n",
        "        \"\"\"\n",
        "        # TODO: Implement initialization of weights\n",
        "\n",
        "        if self.initialize_method == \"random\":\n",
        "            return None * 0.01\n",
        "        if self.initialize_method == \"xavier\":\n",
        "            return None\n",
        "        if self.initialize_method == \"he\":\n",
        "            return None\n",
        "        else:\n",
        "            raise ValueError(\"Invalid initialization method\")\n",
        "\n",
        "    def initialize_bias(self):\n",
        "        \"\"\"\n",
        "        Initialize bias.\n",
        "        returns:\n",
        "            bias: initialized bias with shape: (1, 1, 1, out_channels)\n",
        "\n",
        "        \"\"\"\n",
        "        # TODO: Implement initialization of bias\n",
        "        return None\n",
        "\n",
        "    def target_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "        Calculate the shape of the output of the convolutional layer.\n",
        "        args:\n",
        "            input_shape: shape of the input to the convolutional layer\n",
        "        returns:\n",
        "            target_shape: shape of the output of the convolutional layer\n",
        "        \"\"\"\n",
        "        # TODO: Implement calculation of target shape\n",
        "        H = None\n",
        "        W = None\n",
        "        return (H, W)\n",
        "\n",
        "    def pad(self, A, padding, pad_value=0):\n",
        "        \"\"\"\n",
        "        Pad the input with zeros.\n",
        "        args:\n",
        "            A: input to be padded\n",
        "            padding: tuple of padding for height and width\n",
        "            pad_value: value to pad with\n",
        "        returns:\n",
        "            A_padded: padded input\n",
        "        \"\"\"\n",
        "        A_padded = np.pad(A, ((0, 0), (padding[0], padding[0]), (padding[1], padding[1]), (0, 0)), mode=\"constant\", constant_values=(pad_value, pad_value))\n",
        "        return A_padded\n",
        "\n",
        "    def single_step_convolve(self, a_slic_prev, W, b):\n",
        "        \"\"\"\n",
        "        Convolve a slice of the input with the kernel.\n",
        "        args:\n",
        "            a_slic_prev: slice of the input data\n",
        "            W: kernel\n",
        "            b: bias\n",
        "        returns:\n",
        "            Z: convolved value\n",
        "        \"\"\"\n",
        "        # TODO: Implement single step convolution\n",
        "        Z = None    # hint: element-wise multiplication\n",
        "        Z = None    # hint: sum over all elements\n",
        "        Z = None    # hint: add bias as type float using np.float(None)\n",
        "        return Z\n",
        "\n",
        "    def forward(self, A_prev):\n",
        "        \"\"\"\n",
        "        Forward pass for convolutional layer.\n",
        "            args:\n",
        "                A_prev: activations from previous layer (or input data)\n",
        "                A_prev.shape = (batch_size, H_prev, W_prev, C_prev)\n",
        "            returns:\n",
        "                A: output of the convolutional layer\n",
        "        \"\"\"\n",
        "        # TODO: Implement forward pass\n",
        "        W, b = None\n",
        "        (batch_size, H_prev, W_prev, C_prev) = None\n",
        "        (kernel_size_h, kernel_size_w, C_prev, C) = None\n",
        "        stride_h, stride_w = None\n",
        "        padding_h, padding_w = None\n",
        "        H, W = None\n",
        "        Z = None\n",
        "        A_prev_pad = None # hint: use self.pad()\n",
        "        for i in range(None):\n",
        "            for h in range(None):\n",
        "                h_start = None\n",
        "                h_end = h_start + None\n",
        "                for w in range(None):\n",
        "                    w_start = None\n",
        "                    w_end = w_start + None\n",
        "                    for c in range(None):\n",
        "                        a_slice_prev = A_prev_pad[i, h_start:h_end, w_start:w_end, :]\n",
        "                        Z[i, h, w, c] = None # hint: use self.single_step_convolve()\n",
        "        return Z\n",
        "\n",
        "    def backward(self, dZ, A_prev):\n",
        "        \"\"\"\n",
        "        Backward pass for convolutional layer.\n",
        "        args:\n",
        "            dZ: gradient of the cost with respect to the output of the convolutional layer\n",
        "            A_prev: activations from previous layer (or input data)\n",
        "            A_prev.shape = (batch_size, H_prev, W_prev, C_prev)\n",
        "        returns:\n",
        "            dA_prev: gradient of the cost with respect to the input of the convolutional layer\n",
        "            gradients: list of gradients with respect to the weights and bias\n",
        "        \"\"\"\n",
        "        # TODO: Implement backward pass\n",
        "        W, b = None\n",
        "        (batch_size, H_prev, W_prev, C_prev) = None\n",
        "        (kernel_size_h, kernel_size_w, C_prev, C) = None\n",
        "        stride_h, stride_w = None\n",
        "        padding_h, padding_w = None\n",
        "        H, W = None\n",
        "        dA_prev = None  # hint: same shape as A_prev\n",
        "        dW = None    # hint: same shape as W\n",
        "        db = None    # hint: same shape as b\n",
        "        A_prev_pad = None # hint: use self.pad()\n",
        "        dA_prev_pad = None # hint: use self.pad()\n",
        "        for i in range(None):\n",
        "            a_prev_pad = A_prev_pad[i]\n",
        "            da_prev_pad = dA_prev_pad[i]\n",
        "            for h in range(None):\n",
        "                for w in range(None):\n",
        "                    for c in range(None):\n",
        "                        h_start = None\n",
        "                        h_end = h_start + None\n",
        "                        w_start = None\n",
        "                        w_end = w_start + None\n",
        "                        a_slice = a_prev_pad[h_start:h_end, w_start:w_end, :]\n",
        "                        da_prev_pad += None # hint: use element-wise multiplication of dZ and W\n",
        "                        dW[..., c] += None # hint: use element-wise multiplication of dZ and a_slice\n",
        "                        db[..., c] += None # hint: use dZ\n",
        "            dA_prev[i, :, :, :] = None # hint: remove padding (trick: pad:-pad)\n",
        "        grads = [dW, db]\n",
        "        return dA_prev, grads\n",
        "\n",
        "    def update_parameters(self, optimizer, grads):\n",
        "        \"\"\"\n",
        "        Update parameters of the convolutional layer.\n",
        "        args:\n",
        "            optimizer: optimizer to use for updating parameters\n",
        "            grads: list of gradients with respect to the weights and bias\n",
        "        \"\"\"\n",
        "        self.parameters = optimizer.update(grads, self.name)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "jJ91i3ozSo27"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class MaxPool2D:\n",
        "    def __init__(self, kernel_size=(3, 3), stride=(1, 1), mode=\"max\"):\n",
        "        \"\"\"\n",
        "        Max pooling layer.\n",
        "            args:\n",
        "                kernel_size: size of the kernel\n",
        "                stride: stride of the kernel\n",
        "                mode: max or average\n",
        "            Question:Why we don't need to set name for the layer?\n",
        "        \"\"\"\n",
        "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
        "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
        "        self.mode = mode\n",
        "\n",
        "    def target_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "        Calculate the shape of the output of the layer.\n",
        "            args:\n",
        "                input_shape: shape of the input\n",
        "            returns:\n",
        "                output_shape: shape of the output\n",
        "        \"\"\"\n",
        "        # TODO: Implement target_shape\n",
        "        H = None\n",
        "        W = None\n",
        "        return H, W\n",
        "\n",
        "    def forward(self, A_prev):\n",
        "        \"\"\"\n",
        "        Forward pass for max pooling layer.\n",
        "            args:\n",
        "                A_prev: activations from previous layer (or input data)\n",
        "            returns:\n",
        "                A: output of the max pooling layer\n",
        "        \"\"\"\n",
        "        # TODO: Implement forward pass for max pooling layer\n",
        "        (batch_size, H_prev, W_prev, C_prev) = None\n",
        "        (f_h, f_w) = None\n",
        "        strideh, stridew = None\n",
        "        H, W = None\n",
        "        A = np.zeros((None, None, None, None))\n",
        "        for i in range(None):\n",
        "            for h in range(None):\n",
        "                h_start = None\n",
        "                h_end = h_start + None\n",
        "                for w in range(None):\n",
        "                    w_start = None\n",
        "                    w_end = w_start + None\n",
        "                    for c in range(None):\n",
        "                        a_prev_slice = A_prev[i, h_start:h_end, w_start:w_end, c]\n",
        "                        if self.mode == \"max\":\n",
        "                            A[i, h, w, c] = None\n",
        "                        elif self.mode == \"average\":\n",
        "                            A[i, h, w, c] = None\n",
        "                        else:\n",
        "                            raise ValueError(\"Invalid mode\")\n",
        "\n",
        "        return A\n",
        "\n",
        "    def create_mask_from_window(self, x):\n",
        "        \"\"\"\n",
        "        Create a mask from an input matrix x, to identify the max entry of x.\n",
        "            args:\n",
        "                x: numpy array\n",
        "            returns:\n",
        "                mask: numpy array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
        "        \"\"\"\n",
        "        # TODO: Implement create_mask_from_window\n",
        "        mask = x == None\n",
        "        return mask\n",
        "\n",
        "    def distribute_value(self, dz, shape):\n",
        "        \"\"\"\n",
        "        Distribute the input value in the matrix of dimension shape.\n",
        "            args:\n",
        "                dz: input scalar\n",
        "                shape: the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
        "            returns:\n",
        "                a: distributed value\n",
        "        \"\"\"\n",
        "        # TODO: Implement distribute_value\n",
        "        (n_H, n_W) = shape\n",
        "        average = None\n",
        "        a = np.ones(shape) * None\n",
        "        return a\n",
        "\n",
        "    def backward(self, dZ, A_prev):\n",
        "        \"\"\"\n",
        "        Backward pass for max pooling layer.\n",
        "            args:\n",
        "                dA: gradient of cost with respect to the output of the max pooling layer\n",
        "                A_prev: activations from previous layer (or input data)\n",
        "            returns:\n",
        "                dA_prev: gradient of cost with respect to the input of the max pooling layer\n",
        "        \"\"\"\n",
        "        # TODO: Implement backward pass for max pooling layer\n",
        "        (f_h, f_w) = self.kernel_size\n",
        "        strideh, stridew = self.stride\n",
        "        batch_size, H_prev, W_prev, C_prev = None\n",
        "        batch_size, H, W, C = None\n",
        "        dA_prev = np.zeros((None, None, None, None))\n",
        "        for i in range(None):\n",
        "            for h in range(None):\n",
        "                for w in range(None):\n",
        "                    for c in range(None):\n",
        "                        h_start = None\n",
        "                        h_end = h_start + None\n",
        "                        w_start = None\n",
        "                        w_end = w_start + None\n",
        "                        if self.mode == \"max\":\n",
        "                            a_prev_slice = A_prev[i, h_start:h_end, w_start:w_end, c]\n",
        "                            mask = self.create_mask_from_window(None)\n",
        "                            dA_prev[i, h_start:h_end, w_start:w_end, c] += np.multiply(None, None)\n",
        "                        elif self.mode == \"average\":\n",
        "                            dz = dZ[i, h, w, c]\n",
        "                            dA_prev[i, h_start:h_end, w_start:w_end, c] += self.distribute_value(None, None)\n",
        "                        else:\n",
        "                            raise ValueError(\"Invalid mode\")\n",
        "        # Don't change the return\n",
        "        return dA_prev, None\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "yrXkTqsFSo29"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class BinaryCrossEntropy:\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def compute(self, y_hat: np.ndarray, y: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Computes the binary cross entropy loss.\n",
        "            args:\n",
        "                y: true labels (n_classes, batch_size)\n",
        "                y_hat: predicted labels (n_classes, batch_size)\n",
        "            returns:\n",
        "                binary cross entropy loss\n",
        "        \"\"\"\n",
        "        # TODO: Implement binary cross entropy loss\n",
        "        batch_size = None\n",
        "        cost = None\n",
        "        return np.squeeze(cost)\n",
        "\n",
        "    def backward(self, y_hat: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes the derivative of the binary cross entropy loss.\n",
        "            args:\n",
        "                y: true labels (n_classes, batch_size)\n",
        "                y_hat: predicted labels (n_classes, batch_size)\n",
        "            returns:\n",
        "                derivative of the binary cross entropy loss\n",
        "        \"\"\"\n",
        "        # hint: use the np.divide function\n",
        "        # TODO: Implement backward pass for binary cross entropy loss\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "GTPLsJn6So2-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "class MeanSquaredError:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def compute(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        computes the mean squared error loss\n",
        "            args:\n",
        "                y_pred: predicted labels (n_classes, batch_size)\n",
        "                y_true: true labels (n_classes, batch_size)\n",
        "            returns:\n",
        "                mean squared error loss\n",
        "        \"\"\"\n",
        "        # TODO: Implement mean squared error loss\n",
        "        batch_size = None\n",
        "        cost = None\n",
        "        return np.squeeze(cost)\n",
        "\n",
        "    def backward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        computes the derivative of the mean squared error loss\n",
        "            args:\n",
        "                y_pred: predicted labels (n_classes, batch_size)\n",
        "                y_true: true labels (n_classes, batch_size)\n",
        "            returns:\n",
        "                derivative of the mean squared error loss\n",
        "        \"\"\"\n",
        "        # TODO: Implement backward pass for mean squared error loss\n",
        "        return None"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wMQ02wZpSo2-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Implement the gradient descent optimizer\n",
        "class GD:\n",
        "    def __init__(self, layers_list: dict, learning_rate: float):\n",
        "        \"\"\"\n",
        "        Gradient Descent optimizer.\n",
        "            args:\n",
        "                layers_list: dictionary of layers name and layer object\n",
        "                learning_rate: learning rate\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.layers = layers_list\n",
        "\n",
        "    def update(self, grads, name):\n",
        "        \"\"\"\n",
        "        Update the parameters of the layer.\n",
        "            args:\n",
        "                grads: list of gradients for the weights and bias\n",
        "                name: name of the layer\n",
        "            returns:\n",
        "                params: list of updated parameters\n",
        "        \"\"\"\n",
        "        layer = self.layers[None]\n",
        "        params = []\n",
        "        #TODO: Implement gradient descent update\n",
        "        for None in range(len(grads)):\n",
        "            params.append(None - None * None)\n",
        "        return params"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "DF3ktgKBSo2-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Implement Adam optimizer\n",
        "class Adam:\n",
        "    def __init__(self, layers_list, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.layers = layers_list\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.V = {}\n",
        "        self.S = {}\n",
        "        for None in layers_list:\n",
        "            # TODO: Initialize V and S for each layer (v and s are lists of zeros with the same shape as the parameters)\n",
        "            v = [None for p in layers_list[None].parameters]\n",
        "            s = [None for p in layers_list[None].parameters]\n",
        "            self.V[None] = v\n",
        "            self.S[None] = s\n",
        "\n",
        "    def update(self, grads, name, epoch):\n",
        "        layer = self.layers[None]\n",
        "        params = []\n",
        "        # TODO: Implement Adam update\n",
        "        for None in range(len(grads)):\n",
        "            self.V[None][None] = None * None + (1 - None) * None\n",
        "            self.S[None][None] = None * None  +(1 - None) * np.square(None)\n",
        "            self.V[None][None] /= (1 - np.power(self.beta1, epoch)) # TODO: correct V\n",
        "            self.S[None][None] /= (1 - np.power(self.beta2, epoch)) # TODO: correct S\n",
        "            params.append(None - None * None / (np.sqrt(None) + None))\n",
        "        return params"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "GUmDHklwSo2_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Activation:\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Forward pass for activation function.\n",
        "            args:\n",
        "                Z: input to the activation function\n",
        "            returns:\n",
        "                A: output of the activation function\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, dA: np.ndarray, Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Backward pass for activation function.\n",
        "            args:\n",
        "                dA: derivative of the cost with respect to the activation\n",
        "                Z: input to the activation function\n",
        "            returns:\n",
        "                derivative of the cost with respect to Z\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class Sigmoid(Activation):\n",
        "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Sigmoid activation function.\n",
        "            args:\n",
        "            x: input to the activation function\n",
        "            returns:\n",
        "                sigmoid(x)\n",
        "        \"\"\"\n",
        "        # TODO: Implement sigmoid activation function\n",
        "        A = None\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA: np.ndarray, Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Backward pass for sigmoid activation function.\n",
        "            args:\n",
        "                dA: derivative of the cost with respect to the activation\n",
        "                Z: input to the activation function\n",
        "            returns:\n",
        "                derivative of the cost with respect to Z\n",
        "        \"\"\"\n",
        "        A = self.forward(Z)\n",
        "        # TODO: Implement backward pass for sigmoid activation function\n",
        "        dZ = None\n",
        "        return dZ\n",
        "\n",
        "\n",
        "class ReLU(Activation):\n",
        "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        ReLU activation function.\n",
        "            args:\n",
        "                x: input to the activation function\n",
        "            returns:\n",
        "                relu(x)\n",
        "        \"\"\"\n",
        "        # TODO: Implement ReLU activation function\n",
        "        A = None\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA: np.ndarray, Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Backward pass for ReLU activation function.\n",
        "            args:\n",
        "                dA: derivative of the cost with respect to the activation\n",
        "                Z: input to the activation function\n",
        "            returns:\n",
        "                derivative of the cost with respect to Z\n",
        "        \"\"\"\n",
        "        # TODO: Implement backward pass for ReLU activation function\n",
        "        dZ = None\n",
        "        dZ[Z <= 0] = 0\n",
        "\n",
        "        return dZ\n",
        "\n",
        "\n",
        "\n",
        "class Tanh(Activation):\n",
        "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Tanh activation function.\n",
        "            args:\n",
        "                x: input to the activation function\n",
        "            returns:\n",
        "                tanh(x)\n",
        "        \"\"\"\n",
        "        # TODO: Implement tanh activation function\n",
        "        A = None\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA: np.ndarray, Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Backward pass for tanh activation function.\n",
        "            args:\n",
        "                dA: derivative of the cost with respect to the activation\n",
        "                Z: input to the activation function\n",
        "            returns:\n",
        "                derivative of the cost with respect to Z\n",
        "        \"\"\"\n",
        "        A = self.forward(Z)\n",
        "        # TODO: Implement backward pass for tanh activation function\n",
        "        dZ = None\n",
        "        return dZ\n",
        "\n",
        "class LinearActivation(Activation):\n",
        "    def linear(Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Linear activation function.\n",
        "            args:\n",
        "                x: input to the activation function\n",
        "            returns:\n",
        "                x\n",
        "        \"\"\"\n",
        "        # TODO: Implement linear activation function\n",
        "        A = None\n",
        "        return A\n",
        "\n",
        "    def backward(dA: np.ndarray, Z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Backward pass for linear activation function.\n",
        "            args:\n",
        "                dA: derivative of the cost with respect to the activation\n",
        "                Z: input to the activation function\n",
        "            returns:\n",
        "                derivative of the cost with respect to Z\n",
        "        \"\"\"\n",
        "        # TODO: Implement backward pass for linear activation function\n",
        "        dZ = None\n",
        "        return dZ\n",
        "\n",
        "def get_activation(activation: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Returns the activation function and its derivative.\n",
        "        args:\n",
        "            activation: activation function name\n",
        "        returns:\n",
        "            activation function and its derivative\n",
        "    \"\"\"\n",
        "    if activation == 'sigmoid':\n",
        "        return Sigmoid\n",
        "    elif activation == 'relu':\n",
        "        return ReLU\n",
        "    elif activation == 'tanh':\n",
        "        return Tanh\n",
        "    elif activation == 'linear':\n",
        "        return LinearActivation\n",
        "    else:\n",
        "        raise ValueError('Activation function not supported')"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "eVgpl7EPSo2_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self, arch, criterion, optimizer, name=None):\n",
        "        \"\"\"\n",
        "        Initialize the model.\n",
        "        args:\n",
        "            arch: dictionary containing the architecture of the model\n",
        "            criterion: loss\n",
        "            optimizer: optimizer\n",
        "            name: name of the model\n",
        "        \"\"\"\n",
        "        if name is None:\n",
        "            self.model = arch\n",
        "            self.criterion = criterion\n",
        "            self.optimizer = optimizer\n",
        "            self.layers_names = list(arch.keys())\n",
        "        else:\n",
        "            self.model, self.criterion, self.optimizer, self.layers_names = self.load_model(name)\n",
        "\n",
        "    def is_layer(self, layer):\n",
        "        \"\"\"\n",
        "        Check if the layer is a layer.\n",
        "        args:\n",
        "            layer: layer to be checked\n",
        "        returns:\n",
        "            True if the layer is a layer, False otherwise\n",
        "        \"\"\"\n",
        "        # TODO: Implement check if the layer is a layer\n",
        "        return None\n",
        "\n",
        "    def is_activation(self, layer):\n",
        "        \"\"\"\n",
        "        Check if the layer is an activation function.\n",
        "        args:\n",
        "            layer: layer to be checked\n",
        "        returns:\n",
        "            True if the layer is an activation function, False otherwise\n",
        "        \"\"\"\n",
        "        # TODO: Implement check if the layer is an activation\n",
        "        return None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "        args:\n",
        "            x: input to the model\n",
        "        returns:\n",
        "            output of the model\n",
        "        \"\"\"\n",
        "        tmp = []\n",
        "        A = x\n",
        "        # TODO: Implement forward pass through the model\n",
        "        # NOTICE: we have a pattern of layers and activations\n",
        "        for l in range(None):\n",
        "            Z = None\n",
        "            tmp.append(None)    # hint add a copy of Z to tmp\n",
        "            A = None\n",
        "            tmp.append(None)    # hint add a copy of A to tmp\n",
        "        return tmp\n",
        "\n",
        "    def backward(self, dAL, tmp, x):\n",
        "        \"\"\"\n",
        "        Backward pass through the model.\n",
        "        args:\n",
        "            dAL: derivative of the cost with respect to the output of the model\n",
        "            tmp: list containing the intermediate values of Z and A\n",
        "            x: input to the model\n",
        "        returns:\n",
        "            gradients of the model\n",
        "        \"\"\"\n",
        "        dA = dAL\n",
        "        grads = {}\n",
        "        # TODO: Implement backward pass through the model\n",
        "        # NOTICE: we have a pattern of layers and activations\n",
        "        # for from the end to the beginning of the tmp list\n",
        "        for l in range(None):\n",
        "            if l > 2:\n",
        "                Z, A = tmp[l - 1], tmp[l - 2]\n",
        "            else:\n",
        "                Z, A = tmp[l - 1], x\n",
        "            dZ = None\n",
        "            dA, grad = None\n",
        "            grads[self.layers_names[l - 1]] = None\n",
        "        return grads\n",
        "\n",
        "    def update(self, grads):\n",
        "        \"\"\"\n",
        "        Update the model.\n",
        "        args:\n",
        "            grads: gradients of the model\n",
        "        \"\"\"\n",
        "        for None:\n",
        "            if None:    # hint check if the layer is a layer and also is not a maxpooling layer\n",
        "                self.model[None].update(None)\n",
        "\n",
        "    def one_epoch(self, x, y):\n",
        "        \"\"\"\n",
        "        One epoch of training.\n",
        "        args:\n",
        "            x: input to the model\n",
        "            y: labels\n",
        "            batch_size: batch size\n",
        "        returns:\n",
        "            loss\n",
        "        \"\"\"\n",
        "        # TODO: Implement one epoch of training\n",
        "        tmp = None\n",
        "        AL = tmp[None]\n",
        "        loss = None\n",
        "        dAL = None\n",
        "        grads = None\n",
        "        self.update(None)\n",
        "        return loss\n",
        "\n",
        "    def save(self, name):\n",
        "        \"\"\"\n",
        "        Save the model.\n",
        "        args:\n",
        "            name: name of the model\n",
        "        \"\"\"\n",
        "        with open(name, 'wb') as f:\n",
        "            pickle.dump((self.model, self.criterion, self.optimizer, self.layers_names), f)\n",
        "\n",
        "    def load_model(self, name):\n",
        "        \"\"\"\n",
        "        Load the model.\n",
        "        args:\n",
        "            name: name of the model\n",
        "        returns:\n",
        "            model, criterion, optimizer, layers_names\n",
        "        \"\"\"\n",
        "        with open(name, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    def shuffle(self, m, shuffling):\n",
        "        order = list(range(m))\n",
        "        if shuffling:\n",
        "            return np.random.shuffle(order)\n",
        "        return order\n",
        "\n",
        "    def batch(self, X, y, batch_size, index, order):\n",
        "        \"\"\"\n",
        "        Get a batch of data.\n",
        "        args:\n",
        "            X: input to the model\n",
        "            y: labels\n",
        "            batch_size: batch size\n",
        "            index: index of the batch\n",
        "                e.g: if batch_size = 3 and index = 1 then the batch will be from index [3, 4, 5]\n",
        "            order: order of the data\n",
        "        returns:\n",
        "            bx, by: batch of data\n",
        "        \"\"\"\n",
        "        # TODO: Implement batch\n",
        "        last_index = None   # hint last index of the batch check for the last batch\n",
        "        batch = order[None: None]\n",
        "        # NOTICE: inputs are 4 dimensional or 2 demensional\n",
        "        if None:\n",
        "            bx = None\n",
        "            by = None\n",
        "            return None, None\n",
        "        else:\n",
        "            bx = None\n",
        "            by = None\n",
        "            return None, None\n",
        "\n",
        "    def compute_loss(self, X, y, batch_size):\n",
        "        \"\"\"\n",
        "        Compute the loss.\n",
        "        args:\n",
        "            X: input to the model\n",
        "            y: labels\n",
        "            Batch_Size: batch size\n",
        "        returns:\n",
        "            loss\n",
        "        \"\"\"\n",
        "        # TODO: Implement compute loss\n",
        "        m = None\n",
        "        order = None\n",
        "        cost = 0\n",
        "        for b in range(m // batch_size):\n",
        "            bx, by = None\n",
        "            tmp = None\n",
        "            AL = None\n",
        "            cost += None\n",
        "        return cost\n",
        "\n",
        "    def train(self, X, y, epochs, val=None, batch_size=3, shuffling=False, verbose=1, save_after=None):\n",
        "        \"\"\"\n",
        "        Train the model.\n",
        "        args:\n",
        "            X: input to the model\n",
        "            y: labels\n",
        "            epochs: number of epochs\n",
        "            val: validation data\n",
        "            batch_size: batch size\n",
        "            shuffling: if True shuffle the data\n",
        "            verbose: if 1 print the loss after each epoch\n",
        "            save_after: save the model after training\n",
        "        \"\"\"\n",
        "        # TODO: Implement training\n",
        "        train_cost = []\n",
        "        val_cost = []\n",
        "        # NOTICE: if your inputs are 4 dimensional m = X.shape[0] else m = X.shape[1]\n",
        "        m = None\n",
        "        for e in tqdm(1, epochs + 1):\n",
        "            order = self.shuffle(None, None)\n",
        "            cost = 0\n",
        "            for b in range(None):\n",
        "                bx, by = None\n",
        "                cost += None\n",
        "            train_cost.append(None)\n",
        "            if val is not None:\n",
        "                val_cost.append(None)\n",
        "            if verbose != False:\n",
        "                if e % verbose == 0:\n",
        "                    print(\"Epoch {}: train cost = {}\".format(e, cost))\n",
        "                if val is not None:\n",
        "                    print(\"Epoch {}: val cost = {}\".format(e, val_cost[-1]))\n",
        "        if save_after is not None:\n",
        "            self.save(save_after)\n",
        "        return train_cost, val_cost\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the output of the model.\n",
        "        args:\n",
        "            X: input to the model\n",
        "        returns:\n",
        "            predictions\n",
        "        \"\"\"\n",
        "        # TODO: Implement prediction\n",
        "        return None"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qYaAls3zSo2_"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}